{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "52b8ffff",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import uuid\n",
        "import asyncio\n",
        "from litellm import acompletion\n",
        "from aiolimiter import AsyncLimiter\n",
        "\n",
        "\n",
        "def generate_response_id():\n",
        "    \"\"\"Generate a unique ID for the response.\"\"\"\n",
        "    return str(uuid.uuid4())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "73fd4e87",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "# LiteLLM automatically reads API keys from environment variables:\n",
        "# - OPENAI_API_KEY for OpenAI models\n",
        "# - GEMINI_API_KEY for Google Gemini models\n",
        "# - ANTHROPIC_API_KEY for Anthropic models\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "4164c960",
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import AsyncOpenAI\n",
        "\n",
        "async def create_response_with_ai_gateway(model, user_input, response_schema, system_prompt, reasoning_effort=None):\n",
        "    \"\"\"\n",
        "    Create a response using Cornell AI Gateway (async version).\n",
        "    \n",
        "    Args:\n",
        "        model: Model identifier (e.g., \"openai.gpt-4o\", \"deepseek.r1\")\n",
        "        user_input: The user's input message\n",
        "        response_schema: JSON schema for structured output\n",
        "        system_prompt: System prompt for the LLM\n",
        "        reasoning_effort: Reasoning effort level (\"low\", \"medium\", \"high\") - only for reasoning models\n",
        "    \"\"\"\n",
        "    client = AsyncOpenAI(api_key=os.getenv(\"AI_GATEWAY_KEY\"), base_url=\"https://api.ai.it.cornell.edu\")\n",
        "    \n",
        "    # Build the request parameters\n",
        "    params = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_input}\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    if reasoning_effort:\n",
        "        params[\"reasoning_effort\"] = reasoning_effort\n",
        "\n",
        "    params[\"response_format\"] = {\n",
        "            \"type\": \"json_schema\",\n",
        "            \"json_schema\": {\n",
        "                \"name\": \"batch_statement_response\",\n",
        "                \"strict\": True,\n",
        "                \"schema\": response_schema\n",
        "            }\n",
        "    }\n",
        "    \n",
        "    response = await client.chat.completions.create(**params)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "4dcc3a94",
      "metadata": {},
      "outputs": [],
      "source": [
        "async def create_response_for_openai(model, user_input, response_schema, system_prompt, reasoning_config=None):\n",
        "    \"\"\"Create a response using LiteLLM for OpenAI models.\n",
        "    \n",
        "    Args:\n",
        "        model: Model identifier (e.g., \"openai/responses/gpt-5.2\")\n",
        "        user_input: The user's input message\n",
        "        response_schema: JSON schema for structured output\n",
        "        system_prompt: System prompt for the LLM\n",
        "        reasoning_config: Reasoning configuration - can be:\n",
        "            - \"none\" or None for no reasoning\n",
        "            - {\"effort\": \"high\", \"summary\": \"detailed\"} for OpenAI reasoning\n",
        "    \n",
        "    Returns:\n",
        "        LiteLLM response object\n",
        "    \"\"\"\n",
        "    # Handle reasoning config\n",
        "    if reasoning_config is None or reasoning_config == \"none\":\n",
        "        reasoning_effort = \"none\"\n",
        "    else:\n",
        "        reasoning_effort = reasoning_config\n",
        "    \n",
        "    response = await acompletion(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_input}\n",
        "        ],\n",
        "        reasoning_effort=reasoning_effort,\n",
        "        response_format={\n",
        "            \"type\": \"json_schema\",\n",
        "            \"json_schema\": {\n",
        "                \"name\": \"batch_statement_response\",\n",
        "                \"strict\": True,\n",
        "                \"schema\": response_schema\n",
        "            }\n",
        "        }\n",
        "    )\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "ad7780b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "models =[{\n",
        "    \"provider\": \"google\",\n",
        "    \"models\" : [\n",
        "        {\"name\": \"google.gemini-3-flash-preview\", \"reasoning\": \"high\"},\n",
        "        {\"name\": \"google.gemini-3-flash-preview\", \"reasoning\": \"none\"},\n",
        "    ]\n",
        "},\n",
        "{\n",
        "    \"provider\": \"openai\",\n",
        "    \"models\" : [\n",
        "        {\"name\": \"openai.gpt-5.2\", \"reasoning\": {\"effort\": \"high\", \"summary\": \"detailed\"}},\n",
        "        {\"name\": \"openai.gpt-5.2\", \"reasoning\": \"none\"},\n",
        "    ]\n",
        "},\n",
        "{\n",
        "    \"provider\": \"anthropic\",\n",
        "    \"models\" : [\n",
        "        {\"name\": \"anthropic.claude-4.5-haiku\", \"reasoning\": \"high\"},\n",
        "        {\"name\": \"anthropic.claude-4.5-haiku\", \"reasoning\": \"low\"},\n",
        "    ]\n",
        "},\n",
        "{\n",
        "    \"provider\": \"xai\",\n",
        "    \"models\" : [\n",
        "        {\"name\": \"xai.grok-4-fast-reasoning\", \"reasoning\": \"high\"},\n",
        "        {\"name\": \"xai.grok-4-fast-non-reasoning\", \"reasoning\": \"none\"},\n",
        "    ]\n",
        "},\n",
        "\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "router-function",
      "metadata": {},
      "outputs": [],
      "source": [
        "async def create_response(provider, model_name, user_input, response_schema, system_prompt, reasoning_config=None):\n",
        "    \"\"\"Route API calls based on provider.\n",
        "    \n",
        "    Args:\n",
        "        provider: Provider name ('openai', 'google', 'anthropic', 'xai', etc.)\n",
        "        model_name: Full model identifier\n",
        "        user_input: The user's input message\n",
        "        response_schema: JSON schema for structured output\n",
        "        system_prompt: System prompt for the LLM\n",
        "        reasoning_config: Reasoning configuration from model config\n",
        "    \n",
        "    Returns:\n",
        "        Response object from the appropriate API\n",
        "    \"\"\"\n",
        "    if provider == \"openai\":\n",
        "        # For OpenAI, use LiteLLM with openai/responses/ prefix\n",
        "        litellm_model = f\"openai/responses/{model_name.replace('openai.', '')}\"\n",
        "        return await create_response_for_openai(\n",
        "            litellm_model, user_input, response_schema, system_prompt, reasoning_config\n",
        "        )\n",
        "    else:\n",
        "        # For all other providers, use Cornell AI Gateway\n",
        "        # Extract reasoning effort for non-OpenAI models\n",
        "        reasoning_effort = None\n",
        "        if reasoning_config and reasoning_config != \"none\":\n",
        "            if isinstance(reasoning_config, str):\n",
        "                reasoning_effort = reasoning_config\n",
        "            elif isinstance(reasoning_config, dict):\n",
        "                reasoning_effort = reasoning_config.get(\"effort\")\n",
        "        \n",
        "        return await create_response_with_ai_gateway(\n",
        "            model_name, user_input, response_schema, system_prompt, reasoning_effort\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "f8a3641c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def construct_batch_response_schema(statements, response_options):\n",
        "    \"\"\"Construct JSON schema for batch response with all statements.\n",
        "    \n",
        "    Args:\n",
        "        statements: List of statement dicts with 'id' and 'prompt' keys\n",
        "        response_options: List of valid response options\n",
        "        \n",
        "    Returns:\n",
        "        JSON schema for structured output\n",
        "    \"\"\"\n",
        "    statement_ids = [stmt['id'] for stmt in statements]\n",
        "    statement_texts = [stmt['prompt'] for stmt in statements]\n",
        "    \n",
        "    return {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"responses\": {\n",
        "                \"type\": \"array\",\n",
        "                \"description\": \"Array of responses for each statement\",\n",
        "                \"items\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"question_id\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"The ID of the statement being responded to\",\n",
        "                            \"enum\": statement_ids\n",
        "                        },\n",
        "                        \"input_statement\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"The exact statement text being responded to\",\n",
        "                            \"enum\": statement_texts\n",
        "                        },\n",
        "                        \"response\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"The response to the statement\",\n",
        "                            \"enum\": response_options\n",
        "                        }\n",
        "                    },\n",
        "                    \"required\": [\"question_id\", \"input_statement\", \"response\"],\n",
        "                    \"additionalProperties\": False\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"responses\"],\n",
        "        \"additionalProperties\": False\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "62524b97",
      "metadata": {},
      "outputs": [],
      "source": [
        "def verify_batch_responses(responses, statement_ids, response_options):\n",
        "    \"\"\"Validate all responses in a batch.\n",
        "    \n",
        "    Args:\n",
        "        responses: List of response dicts with 'question_id' and 'response' keys\n",
        "        statement_ids: List of expected statement IDs\n",
        "        response_options: List of valid response options\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (is_valid, error_message)\n",
        "    \"\"\"\n",
        "    if not responses:\n",
        "        return False, \"No responses received\"\n",
        "    \n",
        "    # Check if we got the right number of responses\n",
        "    if len(responses) != len(statement_ids):\n",
        "        return False, f\"Expected {len(statement_ids)} responses, got {len(responses)}\"\n",
        "    \n",
        "    # Check all question_ids are present and responses are valid\n",
        "    received_ids = set()\n",
        "    for resp in responses:\n",
        "        qid = resp.get('question_id')\n",
        "        answer = resp.get('response')\n",
        "        \n",
        "        if qid not in statement_ids:\n",
        "            return False, f\"Unexpected question_id: {qid}\"\n",
        "        \n",
        "        if qid in received_ids:\n",
        "            return False, f\"Duplicate question_id: {qid}\"\n",
        "        \n",
        "        if answer not in response_options:\n",
        "            return False, f\"Invalid response '{answer}' for {qid}\"\n",
        "        \n",
        "        received_ids.add(qid)\n",
        "    \n",
        "    # Check all expected IDs were received\n",
        "    missing_ids = set(statement_ids) - received_ids\n",
        "    if missing_ids:\n",
        "        return False, f\"Missing responses for: {missing_ids}\"\n",
        "    \n",
        "    return True, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "7f7abcb5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def construct_batch_user_input(statements):\n",
        "    \"\"\"Construct user input containing all statements for batch processing.\n",
        "    \n",
        "    Args:\n",
        "        statements: List of statement dicts with 'id' and 'prompt' keys\n",
        "        \n",
        "    Returns:\n",
        "        Formatted string with all statements\n",
        "    \"\"\"\n",
        "    prompt = \"Please respond to each of the following statements:\\n\\n\"\n",
        "    for stmt in statements:\n",
        "        prompt += f\"[{stmt['id']}]: {stmt['prompt']}\\n\"\n",
        "    return prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "67512218",
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_response(folder, id, data):\n",
        "    \"\"\"Save full LLM response to responses/ folder.\n",
        "    \n",
        "    Args:\n",
        "        response_json: The full response object as a dictionary\n",
        "        \n",
        "    Returns:\n",
        "        str: The response ID\n",
        "    \"\"\"\n",
        "    filepath = f\"{folder}/{id}.json\"\n",
        "    with open(filepath, 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "    return id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "e8326154",
      "metadata": {},
      "outputs": [],
      "source": [
        "def append_to_csv(filepath, row_data, statement_ids):\n",
        "    \"\"\"Append a single row to CSV, create with headers if file doesn't exist.\n",
        "    \n",
        "    Args:\n",
        "        filepath: Path to the CSV file\n",
        "        row_data: Dictionary containing row data with question IDs as keys\n",
        "        statement_ids: List of statement IDs for column ordering\n",
        "    \"\"\"\n",
        "    # Build fieldnames: metadata columns + question ID columns + response ID\n",
        "    fieldnames = ['country', 'model', 'reasoning', 'run_number', 'attempt'] + statement_ids + ['llm_response_id']\n",
        "    file_exists = os.path.exists(filepath)\n",
        "    \n",
        "    with open(filepath, 'a', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "        if not file_exists:\n",
        "            writer.writeheader()\n",
        "        writer.writerow(row_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "3034ed11",
      "metadata": {},
      "outputs": [],
      "source": [
        "async def process_all_statements_batch(statements, run_number, response_options, \n",
        "                                        response_schema, system_prompt, csv_filepath, \n",
        "                                        provider, model_name, reasoning_config, max_attempts,\n",
        "                                        country=None, response_folder=None):\n",
        "    \"\"\"Process all statements in a single batch with retry logic.\n",
        "    \n",
        "    Args:\n",
        "        statements: List of statement dicts with 'id' and 'prompt'\n",
        "        run_number: Current run iteration number\n",
        "        response_options: List of valid response options\n",
        "        response_schema: JSON schema for response validation\n",
        "        system_prompt: System prompt for the LLM\n",
        "        csv_filepath: Path to CSV file for logging\n",
        "        provider: Provider name for API routing\n",
        "        model_name: The model identifier\n",
        "        reasoning_config: Reasoning configuration for the model\n",
        "        max_attempts: Maximum retry attempts for invalid responses\n",
        "        country: Country name for country-based processing (optional)\n",
        "        response_folder: Folder to save response JSON files (optional)\n",
        "        \n",
        "    Returns:\n",
        "        bool: True if successful, False if all attempts failed\n",
        "    \"\"\"\n",
        "    statement_ids = [stmt['id'] for stmt in statements]\n",
        "    user_input = construct_batch_user_input(statements)\n",
        "    \n",
        "    for attempt in range(1, max_attempts + 1):\n",
        "        try:\n",
        "            response = await create_response(\n",
        "                provider, model_name, user_input, response_schema, system_prompt, reasoning_config\n",
        "            )\n",
        "            \n",
        "            # Handle both LiteLLM and OpenAI response formats\n",
        "            if hasattr(response, 'model_dump'):\n",
        "                response_json = response.model_dump()\n",
        "            else:\n",
        "                response_json = response.to_dict() if hasattr(response, 'to_dict') else dict(response)\n",
        "            \n",
        "            # Parse the response content\n",
        "            content = response_json[\"choices\"][0][\"message\"][\"content\"]\n",
        "            output = json.loads(content)\n",
        "            \n",
        "            # Generate unique ID and save full response\n",
        "            response_id = generate_response_id()\n",
        "            if response_folder:\n",
        "                save_response(response_folder, response_id, response_json)\n",
        "            \n",
        "            # Validate all responses in the batch\n",
        "            responses = output.get('responses', [])\n",
        "            is_valid, error_msg = verify_batch_responses(responses, statement_ids, response_options)\n",
        "            \n",
        "            if is_valid:\n",
        "                # Format reasoning config for CSV (extract effort from dict if needed)\n",
        "                if not reasoning_config:\n",
        "                    reasoning_str = 'none'\n",
        "                elif isinstance(reasoning_config, dict):\n",
        "                    reasoning_str = reasoning_config.get('effort', 'none')\n",
        "                else:\n",
        "                    reasoning_str = str(reasoning_config)\n",
        "                \n",
        "                # Build row data with question IDs as columns\n",
        "                row_data = {\n",
        "                    'country': country,\n",
        "                    'model': model_name,\n",
        "                    'reasoning': reasoning_str,\n",
        "                    'run_number': run_number,\n",
        "                    'attempt': attempt,\n",
        "                    'llm_response_id': response_id\n",
        "                }\n",
        "                \n",
        "                # Add each question's response as a column\n",
        "                for resp in responses:\n",
        "                    row_data[resp['question_id']] = resp['response']\n",
        "                \n",
        "                append_to_csv(csv_filepath, row_data, statement_ids)\n",
        "                return True\n",
        "            \n",
        "            print(f\"Validation failed for {country}/{model_name}, attempt {attempt}/{max_attempts}: {error_msg}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing batch for {country}/{model_name}, attempt {attempt}/{max_attempts}: {str(e)}\")\n",
        "    \n",
        "    print(f\"FAILED: {country}/{model_name} after {max_attempts} attempts\")\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "300a7c0d",
      "metadata": {},
      "outputs": [],
      "source": [
        "async def process_batch_with_rate_limit(rate_limiter, statements, run_number, \n",
        "                                         response_options, response_schema, system_prompt,\n",
        "                                         csv_filepath, provider, model_name, reasoning_config,\n",
        "                                         max_attempts, country=None, response_folder=None):\n",
        "    \"\"\"Wrapper that applies rate limiting before processing a batch.\n",
        "    \n",
        "    Args:\n",
        "        rate_limiter: AsyncLimiter instance for rate limiting\n",
        "        (all other args same as process_all_statements_batch)\n",
        "        \n",
        "    Returns:\n",
        "        bool: True if successful, False if all attempts failed\n",
        "    \"\"\"\n",
        "    async with rate_limiter:\n",
        "        return await process_all_statements_batch(\n",
        "            statements, run_number, response_options, response_schema, system_prompt,\n",
        "            csv_filepath, provider, model_name, reasoning_config, max_attempts,\n",
        "            country, response_folder\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "09cdacf4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def construct_country_system_prompt(country):\n",
        "    return f\"\"\"\n",
        "        You are a primary or secondary school teacher from {country} responding to a survey about the use of artificial intelligence (AI) in education.\n",
        "        Please indicate your level of agreement with the list of statements presented in the survey.\n",
        "        In the survey, AI is defined as below:\n",
        "        'Artificial intelligence' is the capacity for computers to perform tasks traditionally thought to involve human intelligence. This can include making predictions, suggesting decisions, or generating text.\n",
        "\n",
        "        Please respond with only one of the response options for each statement.\n",
        "        Response options:\n",
        "        ['Strongly disagree', 'Disagree', 'Agree', 'Strongly agree', 'I don't know']\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "11125c49",
      "metadata": {},
      "outputs": [],
      "source": [
        "async def process_all_countries(countries, statements, response_options, \n",
        "                                 csv_filepath, response_folder, num_runs, model_configs, max_attempts,\n",
        "                                 requests_per_minute=60):\n",
        "    \"\"\"Process all statements as a batch for all countries and all models with rate limiting.\n",
        "    \n",
        "    Args:\n",
        "        countries: List of country names\n",
        "        statements: List of statement dicts with 'id' and 'prompt'\n",
        "        response_options: List of valid response options\n",
        "        csv_filepath: Path to CSV file for logging\n",
        "        response_folder: Folder to save response JSON files\n",
        "        num_runs: Number of times to run for each country\n",
        "        model_configs: List of provider configs with format:\n",
        "            [{\\\"provider\\\": \\\"google\\\", \\\"models\\\": [{\\\"name\\\": \\\"...\\\", \\\"reasoning\\\": \\\"...\\\"}]}]\n",
        "        max_attempts: Maximum retry attempts for invalid responses\n",
        "        requests_per_minute: Rate limit for API requests (default 60)\n",
        "    \"\"\"\n",
        "    # Create rate limiter\n",
        "    rate_limiter = AsyncLimiter(requests_per_minute, 60)  # X requests per 60 seconds\n",
        "    \n",
        "    # Get statement IDs for schema\n",
        "    statement_ids = [stmt['id'] for stmt in statements]\n",
        "    total_countries = len(countries)\n",
        "    total_statements = len(statements)\n",
        "    \n",
        "    # Build response schema for batch processing\n",
        "    response_schema = construct_batch_response_schema(statements, response_options)\n",
        "    \n",
        "    # Flatten model configs to list of (provider, model_name, reasoning_config) tuples\n",
        "    all_models = [\n",
        "        (pc['provider'], m['name'], m.get('reasoning'))\n",
        "        for pc in model_configs \n",
        "        for m in pc['models']\n",
        "    ]\n",
        "    total_models = len(all_models)\n",
        "    total_tasks_per_country = total_models * num_runs\n",
        "    \n",
        "    print(f\"Configuration: {total_models} models x {num_runs} runs = {total_tasks_per_country} tasks per country\")\n",
        "    print(f\"Total countries: {total_countries}\")\n",
        "    print(f\"Rate limit: {requests_per_minute} requests/minute\")\n",
        "    \n",
        "    for country_idx, country in enumerate(countries, 1):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"[{country_idx}/{total_countries}] Processing country: {country}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        system_prompt = construct_country_system_prompt(country)\n",
        "        \n",
        "        print(f\"  Processing {total_tasks_per_country} tasks in parallel ({total_models} models x {num_runs} runs)...\")\n",
        "        \n",
        "        # Create tasks for ALL (model, run) combinations to process in parallel\n",
        "        tasks = [\n",
        "            process_batch_with_rate_limit(\n",
        "                rate_limiter, statements, run_number, response_options,\n",
        "                response_schema, system_prompt, csv_filepath,\n",
        "                provider, model_name, reasoning_config, max_attempts,\n",
        "                country, response_folder\n",
        "            )\n",
        "            for provider, model_name, reasoning_config in all_models\n",
        "            for run_number in range(1, num_runs + 1)\n",
        "        ]\n",
        "        \n",
        "        # Execute all tasks in parallel (rate limiter controls throughput)\n",
        "        results = await asyncio.gather(*tasks)\n",
        "        \n",
        "        success_count = sum(results)\n",
        "        print(f\"  Completed: {success_count}/{total_tasks_per_country} tasks successful\")\n",
        "    \n",
        "    print(f\"\\nCompleted processing {total_countries} countries x {total_models} models x {num_runs} runs -> {csv_filepath}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ca2fbe3",
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_gen_config_details_to_file(output_folder, countries, statements, response_schema, num_runs, models, max_attempts, prompt, user_input):\n",
        "    with open(f\"{output_folder}/gen_config_details.txt\", \"w\") as f:\n",
        "        f.write(f\"Countries: {countries}\\n\")\n",
        "        f.write(f\"Statements: {statements}\\n\")\n",
        "        f.write(f\"Response schema: {response_schema}\\n\")\n",
        "        f.write(f\"Num runs: {num_runs}\\n\")\n",
        "        f.write(f\"Models: {models}\\n\")\n",
        "        f.write(f\"Max attempts: {max_attempts}\\n\")\n",
        "        f.write(f\"System Prompt: {prompt}\\n\")\n",
        "        f.write(f\"User input: {user_input}\\n\")\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "93987cf7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "55\n"
          ]
        }
      ],
      "source": [
        "countries_file = \"country_language_list.csv\"\n",
        "\n",
        "countries_df = pd.read_csv(countries_file)\n",
        "\n",
        "unique_list_of_countries = countries_df[\"CNTRY_FULL\"].unique().tolist()\n",
        "\n",
        "print(len(unique_list_of_countries))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "47bdb677",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration: 8 models x 10 runs = 80 tasks per country\n",
            "Total countries: 2\n",
            "Rate limit: 100 requests/minute\n",
            "\n",
            "============================================================\n",
            "[1/2] Processing country: United Arab Emirates\n",
            "============================================================\n",
            "  Processing 80 tasks in parallel (8 models x 10 runs)...\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "  Completed: 80/80 tasks successful\n",
            "\n",
            "============================================================\n",
            "[2/2] Processing country: Australia\n",
            "============================================================\n",
            "  Processing 80 tasks in parallel (8 models x 10 runs)...\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "  Completed: 80/80 tasks successful\n",
            "\n",
            "Completed processing 2 countries x 8 models x 10 runs -> output/batch_processing/20260130_192947/all.csv\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "# Model configurations with provider and reasoning settings\n",
        "# Uses the 'models' variable defined earlier with format:\n",
        "# [{\"provider\": \"...\", \"models\": [{\"name\": \"...\", \"reasoning\": \"...\"}]}]\n",
        "MODEL_CONFIGS = models  # Reference the models config defined earlier\n",
        "\n",
        "MAX_ATTEMPTS = 3  # Maximum retry attempts for invalid responses\n",
        "NUM_RUNS = 10      \n",
        "\n",
        "# Rate limiting - requests per minute\n",
        "# Adjust based on your API tier:\n",
        "#   - OpenAI: 500-10000 RPM depending on tier\n",
        "#   - Gemini: 60-1000 RPM depending on tier  \n",
        "#   - Anthropic: 50-4000 RPM depending on tier\n",
        "REQUESTS_PER_MINUTE = 100\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "response_options = [\"Strongly disagree\", \"Disagree\", \"Agree\", \"Strongly agree\", \"I don't know\"]\n",
        "\n",
        "list_of_statements = json.load(open(\"questions/usa_english.json\", encoding='utf-8'))[\"questions\"][0][\"questions\"]\n",
        "\n",
        "# Get statement IDs for schema construction\n",
        "statement_ids = [stmt['id'] for stmt in list_of_statements]\n",
        "\n",
        "output_folder = f\"output/batch_processing/{timestamp}\"\n",
        "output_csv_file = f\"{output_folder}/all.csv\"\n",
        "response_folder = f\"{output_folder}/responses\"\n",
        "\n",
        "countries = unique_list_of_countries[:2]\n",
        "\n",
        "# Create output folder if not exists\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Create response folder if not exists\n",
        "os.makedirs(response_folder, exist_ok=True)\n",
        "\n",
        "sample_user_input = construct_batch_user_input(list_of_statements)\n",
        "\n",
        "# Save configuration details\n",
        "save_gen_config_details_to_file(output_folder, countries, list_of_statements, \n",
        "                                 construct_batch_response_schema(list_of_statements, response_options), \n",
        "                                 NUM_RUNS, MODEL_CONFIGS, MAX_ATTEMPTS, construct_country_system_prompt(\"{country_name}\"), sample_user_input)\n",
        "\n",
        "# Run batch processing for all countries and models\n",
        "await process_all_countries(\n",
        "    countries, list_of_statements, response_options, \n",
        "    output_csv_file, response_folder, NUM_RUNS, MODEL_CONFIGS, MAX_ATTEMPTS,\n",
        "    REQUESTS_PER_MINUTE\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
