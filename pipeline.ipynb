{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "52b8ffff",
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai\n",
        "import json\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "73fd4e87",
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import AsyncOpenAI\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "client = AsyncOpenAI(api_key=api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "4dcc3a94",
      "metadata": {},
      "outputs": [],
      "source": [
        "async def create_response(model, input, response_schema, system_prompt):\n",
        "    response = await client.responses.create(\n",
        "        model=model,\n",
        "        input=input,\n",
        "        instructions=system_prompt,\n",
        "        text={\n",
        "            \"format\": {\n",
        "                \"name\": \"statement_response\",\n",
        "                \"type\": \"json_schema\",\n",
        "                \"strict\": True,\n",
        "                \"schema\": response_schema\n",
        "            }\n",
        "        }\n",
        "    )\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "f8a3641c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def construct_response_schema(response_options):\n",
        "    return {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"input_statement\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The exact statement being responded to\"\n",
        "            },\n",
        "            \"response\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The response to the statement\",\n",
        "                \"enum\": response_options\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"input_statement\", \"response\"],\n",
        "        \"additionalProperties\": False\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "6210d0fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "def construct_generic_system_prompt(language, ai_definition, response_options):\n",
        "    return f\"\"\"\n",
        "    You are a primary or secondary school teacher. You have to respond to a survey statement in {language}.\n",
        "    Please indicate your level of agreement with the statement about the use of artificial intelligence (AI) in education.\n",
        "    In the survey, AI is defined as below in {language}:\n",
        "    {ai_definition}\n",
        "\n",
        "    Please respond with only one of the response options for each statement.\n",
        "    Response options:\n",
        "    {response_options}\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "62524b97",
      "metadata": {},
      "outputs": [],
      "source": [
        "def verify_response_match_options(response, response_options):\n",
        "    \"\"\"Check if response is in the list of valid options. Returns bool.\"\"\"\n",
        "    return response in response_options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7f7abcb5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def construct_generic_user_input(statement):\n",
        "    return f\"Statement to respond to: {statement}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "9a138ac1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_json_file(file_path, data):\n",
        "    with open(file_path, 'w') as f:\n",
        "        json.dump(data, f, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "67512218",
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_response(response_json):\n",
        "    \"\"\"Save full LLM response to responses/ folder.\n",
        "    \n",
        "    Args:\n",
        "        response_json: The full response object as a dictionary\n",
        "        \n",
        "    Returns:\n",
        "        str: The response ID\n",
        "    \"\"\"\n",
        "    response_id = response_json[\"id\"]\n",
        "    filepath = f\"output/responses/{response_id}.json\"\n",
        "    with open(filepath, 'w', encoding='utf-8') as f:\n",
        "        json.dump(response_json, f, indent=2, ensure_ascii=False)\n",
        "    return response_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "e8326154",
      "metadata": {},
      "outputs": [],
      "source": [
        "def append_to_csv(filepath, row_data):\n",
        "    \"\"\"Append a single row to CSV, create with headers if file doesn't exist.\n",
        "    \n",
        "    Args:\n",
        "        filepath: Path to the CSV file\n",
        "        row_data: Dictionary containing row data\n",
        "    \"\"\"\n",
        "    fieldnames = ['question_id', 'run_number', 'attempt', 'response_from_llm', 'response_score', 'llm_response_id']\n",
        "    file_exists = os.path.exists(filepath)\n",
        "    \n",
        "    with open(filepath, 'a', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "        if not file_exists:\n",
        "            writer.writeheader()\n",
        "        writer.writerow(row_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "3034ed11",
      "metadata": {},
      "outputs": [],
      "source": [
        "async def process_statement(statement, run_number, response_options, response_options_obj, \n",
        "                            response_schema, system_prompt, csv_filepath):\n",
        "    \"\"\"Process a single statement with retry logic.\n",
        "    \n",
        "    Args:\n",
        "        statement: Statement dict with 'id' and 'prompt'\n",
        "        run_number: Current run iteration number\n",
        "        response_options: List of valid response options\n",
        "        response_options_obj: Dict mapping responses to scores\n",
        "        response_schema: JSON schema for response validation\n",
        "        system_prompt: System prompt for the LLM\n",
        "        csv_filepath: Path to CSV file for logging\n",
        "        \n",
        "    Returns:\n",
        "        bool: True if successful, False if all attempts failed\n",
        "    \"\"\"\n",
        "    question_id = statement[\"id\"]\n",
        "    user_input = construct_generic_user_input(statement[\"prompt\"])\n",
        "    \n",
        "    for attempt in range(1, MAX_ATTEMPTS + 1):\n",
        "        response = await create_response(MODEL, user_input, response_schema, system_prompt)\n",
        "        response_json = response.model_dump()\n",
        "        output = json.loads(response_json[\"output\"][0][\"content\"][0][\"text\"])\n",
        "        \n",
        "        # Save full response to responses/ folder\n",
        "        response_id = save_response(response_json)\n",
        "        \n",
        "        # Validate response\n",
        "        if verify_response_match_options(output[\"response\"], response_options):\n",
        "            response_score = response_options_obj[output[\"response\"]]\n",
        "            append_to_csv(csv_filepath, {\n",
        "                'question_id': question_id,\n",
        "                'run_number': run_number,\n",
        "                'attempt': attempt,\n",
        "                'response_from_llm': output[\"response\"],\n",
        "                'response_score': response_score,\n",
        "                'llm_response_id': response_id\n",
        "            })\n",
        "            return True\n",
        "        \n",
        "        print(f\"Invalid response '{output['response']}' for {question_id}, attempt {attempt}/{MAX_ATTEMPTS}\")\n",
        "    \n",
        "    print(f\"FAILED: {question_id} after {MAX_ATTEMPTS} attempts\")\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "402fb0cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "async def process_file(filepath):\n",
        "    \"\"\"Process all statements in a file for NUM_RUNS iterations.\n",
        "    \n",
        "    Args:\n",
        "        filepath: Path to the questions JSON file\n",
        "    \"\"\"\n",
        "    content = json.load(open(filepath, encoding='utf-8'))\n",
        "    \n",
        "    # Extract file name for CSV (e.g., \"questions/usa_english.json\" -> \"generic/usa_english.csv\")\n",
        "    filename = os.path.basename(filepath).replace('.json', '.csv')\n",
        "    csv_filepath = f\"output/generic/{filename}\"\n",
        "    \n",
        "    # Clear existing CSV for fresh run\n",
        "    if os.path.exists(csv_filepath):\n",
        "        os.remove(csv_filepath)\n",
        "    \n",
        "    # Setup from file content\n",
        "    language = content[\"language\"]\n",
        "    ai_definition = content[\"questions\"][0][\"description\"]\n",
        "    response_options_obj = content[\"questions\"][0][\"responses\"]\n",
        "    response_options = list(response_options_obj.keys())\n",
        "    response_schema = construct_response_schema(response_options)\n",
        "    system_prompt = construct_generic_system_prompt(language, ai_definition, response_options)\n",
        "    statements = content[\"questions\"][0][\"questions\"]\n",
        "    \n",
        "    total_statements = len(statements)\n",
        "    \n",
        "    # Run NUM_RUNS times\n",
        "    for run_number in range(1, NUM_RUNS + 1):\n",
        "        print(f\"[{filepath}] Run {run_number}/{NUM_RUNS}\")\n",
        "        for idx, statement in enumerate(statements, 1):\n",
        "            print(f\"  Processing statement {idx}/{total_statements}: {statement['id']}\", end=\" \")\n",
        "            success = await process_statement(\n",
        "                statement, run_number, response_options, response_options_obj,\n",
        "                response_schema, system_prompt, csv_filepath\n",
        "            )\n",
        "            print(\"✓\" if success else \"✗\")\n",
        "    \n",
        "    print(f\"Completed: {filepath} -> {csv_filepath}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "8d16ffa3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[questions/alb_albanian.json] Run 1/2\n",
            "  Processing statement 1/10: TT4G35A ✓\n",
            "  Processing statement 2/10: TT4G35B ✓\n",
            "  Processing statement 3/10: TT4G35C ✓\n",
            "  Processing statement 4/10: TT4G35D ✓\n",
            "  Processing statement 5/10: TT4G35E ✓\n",
            "  Processing statement 6/10: TT4G35F ✓\n",
            "  Processing statement 7/10: TT4G35G ✓\n",
            "  Processing statement 8/10: TT4G35H ✓\n",
            "  Processing statement 9/10: TT4G35I ✓\n",
            "  Processing statement 10/10: TT4G35J ✓\n",
            "[questions/alb_albanian.json] Run 2/2\n",
            "  Processing statement 1/10: TT4G35A ✓\n",
            "  Processing statement 2/10: TT4G35B ✓\n",
            "  Processing statement 3/10: TT4G35C ✓\n",
            "  Processing statement 4/10: TT4G35D ✓\n",
            "  Processing statement 5/10: TT4G35E ✓\n",
            "  Processing statement 6/10: TT4G35F ✓\n",
            "  Processing statement 7/10: TT4G35G ✓\n",
            "  Processing statement 8/10: TT4G35H ✓\n",
            "  Processing statement 9/10: TT4G35I ✓\n",
            "  Processing statement 10/10: TT4G35J ✓\n",
            "Completed: questions/alb_albanian.json -> output/generic/alb_albanian.csv\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "MODEL = \"gpt-4o\"\n",
        "MAX_ATTEMPTS = 3  # Maximum retry attempts for invalid responses\n",
        "NUM_RUNS = 2      # Number of times to run each file\n",
        "\n",
        "FILES = [\n",
        "    # \"questions/usa_english.json\",\n",
        "    \"questions/alb_albanian.json\",\n",
        "]\n",
        "\n",
        "for filepath in FILES:\n",
        "    await process_file(filepath)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
